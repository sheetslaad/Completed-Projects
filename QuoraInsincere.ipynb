{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "641f0a35d903755803d26399eab4ae227a45f56c"
   },
   "source": [
    "## 1001 Final Project: Quora Insincere Questions Classification\n",
    "#### Data: https://www.kaggle.com/c/quora-insincere-questions-classification\n",
    "#### Sheetal Laad, Elliot Silva, Esteban Navarro, Adrian Pearl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c16ed1f1e690bde79c294f0f3a86362ba440a92"
   },
   "source": [
    "1. Run \"Simple Model\" grid search on training/validation set\n",
    "2. Use highest performing \"Simple Model\" for test set\n",
    "3. Neural Network and Embedding modeling and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f59dd29f7286d1c539229f3ff33d924120ed9bb"
   },
   "source": [
    "##### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import string, re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc97843d4421f6ca43355939770a52a3cb6e5c2c"
   },
   "source": [
    "### 1. Run \"Simple Model\" grid search on training/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/train.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(dataset, training_split, test_split):\n",
    "    X = dataset['question_text']\n",
    "    Y = dataset['target']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = training_split, test_size = test_split)\n",
    "    \n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "def data_prep_training_nodownsample(dataset, training_split, test_split):\n",
    "    train, test = train_test_split(dataset, train_size = training_split, test_size = test_split)\n",
    "    #train_downsampled = downsample(train)\n",
    "    \n",
    "    X_train = train['question_text']\n",
    "    Y_train = train['target']\n",
    "    X_test = test['question_text']\n",
    "    Y_test = test['target']\n",
    "    \n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "\n",
    "def model_vectorize(data_used, vectorizer_type, binary_type, ngram, stop_word, model_type):\n",
    "    X_train = data_used[0]\n",
    "    X_test = data_used[1]\n",
    "    Y_train = data_used[2]\n",
    "    Y_test = data_used[3]\n",
    "    \n",
    "    vectorizer = vectorizer_type(binary = binary_type, stop_words = stop_word, ngram_range=ngram)\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train_vectorized = vectorizer.transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    model = model_type\n",
    "    model.fit(X_train_vectorized, Y_train)\n",
    "    \n",
    "    scores = model.predict_proba(X_test_vectorized)[:,1]\n",
    "    precision, recall, thresholds = precision_recall_curve(Y_test, scores)\n",
    "    precision, recall = precision[:-1], recall[:-1]\n",
    "    fscores = 2*np.divide(np.multiply(precision, recall), np.add(precision, recall))\n",
    "    max_fscore = np.nanmax(fscores)\n",
    "    ind_max = fscores.argmax() #not sure this is working\n",
    "    threshold_max = thresholds[ind_max]\n",
    "    \n",
    "    return(max_fscore, threshold_max)\n",
    "\n",
    "\n",
    "def downsample(df):\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df.target==0]\n",
    "    df_minority = df[df.target==1]\n",
    "\n",
    "    # Downsample majority class\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                     replace=False,    # sample without replacement\n",
    "                                     n_samples=df_minority.shape[0],     # to match minority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine minority class with downsampled majority class\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "    \n",
    "    return (df_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'wouldnt':'would not',\n",
    "                'isnt':'is not',\n",
    "                'wouldnt':'would not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'neighbour': 'neighbor',\n",
    "                'humour': 'humor',\n",
    "                'apologise': 'apologize',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'recognise': 'recognize',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'travelled': 'traveled',\n",
    "                'offence': 'offense',\n",
    "                'licence': 'license',\n",
    "                'labour':'labor',\n",
    "                'behaviour': 'behavior',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social_media',\n",
    "                'whatsapp': 'social_media',\n",
    "                'snapchat': 'social_media',\n",
    "                'facebook': 'social_media'\n",
    "\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    # Remove puncuation\n",
    "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"'m\", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    \n",
    "    text = re.sub('[0-9]{5,}', '#####', text)\n",
    "    text = re.sub('[0-9]{4}', '####', text)\n",
    "    text = re.sub('[0-9]{3}', '###', text)\n",
    "    text = re.sub('[0-9]{2}', '##', text)\n",
    "    text = re.sub('[0-9]{1}', '#', text)\n",
    "    \n",
    "    #Replace typical misspells\n",
    "    text = replace_typical_misspell(text)\n",
    "    \n",
    "    #Stem words\n",
    "#     stemmer = SnowballStemmer('english')\n",
    "#     text = ' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
    "\n",
    "    \n",
    "    #Lemmitize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question_text'] = data[\"question_text\"].apply(lambda x: clean(x))\n",
    "noDS_noStem_lem = data_prep_training_nodownsample(data, .9, .1)\n",
    "\n",
    "#create models\n",
    "dataset = [noDS_noStem_lem]\n",
    "vectorizer_type = [CountVectorizer, TfidfVectorizer]\n",
    "binary_type = [True, False]\n",
    "ngram = [(1,2), (1,3), (1,4)]\n",
    "stop_word = [None]\n",
    "model_type = [LogisticRegression(), BernoulliNB()]\n",
    "\n",
    "model_initialize = []\n",
    "for h in dataset:\n",
    "    for i in vectorizer_type:\n",
    "        for j in binary_type:\n",
    "            for k in ngram:\n",
    "                for l in model_type:\n",
    "                    for m in stop_word:\n",
    "                        model_initialize.append(model_vectorize(h, i, j, k, m, l))\n",
    "                    \n",
    "#create labels\n",
    "dataset_label = ['No Downsample No Stemmed Lemmitized']\n",
    "vectorizer_type_label = ['CountV', 'TFIDV']\n",
    "binary_type_label = ['T', 'F']\n",
    "ngram_label = [(1,2), (1,3), (1,4)]\n",
    "stop_word_label = ['None']\n",
    "model_type_label = ['LR', 'NB']\n",
    "\n",
    "labels = []\n",
    "for h in dataset_label:    \n",
    "    for i in vectorizer_type_label:\n",
    "        for j in binary_type_label:\n",
    "            for k in ngram_label:\n",
    "                for l in model_type_label:\n",
    "                    for m in stop_word_label:\n",
    "                        label = '%s %s %s %s %s %s' %(h, i, j, k, m, l)\n",
    "                        labels.append(label)\n",
    "                        \n",
    "fscores = []\n",
    "thresholds = []\n",
    "for i in model_initialize:\n",
    "    fscores.append(i[0])\n",
    "    thresholds.append(i[1])\n",
    "metrics = pd.DataFrame({'label': labels,'fscore': fscores, 'threshold': thresholds})\n",
    "metrics.sort_values(by = ['fscore'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00a638e006ce5cccf326a1400602712c0bb305f0"
   },
   "source": [
    "### 2. Use highest performing \"Simple Model\" for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5732baa77e1cfef2e79e17396b6253c4c008a78f"
   },
   "source": [
    "##### Functions for Feature Engineering and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def data_prep_training_nodownsample(dataset, training_split, test_split):\n",
    "    train, test = train_test_split(dataset, train_size = training_split, test_size = test_split, random_state = 0)\n",
    "    #train_downsampled = downsample(train)\n",
    "    \n",
    "    X_train = train['question_text']\n",
    "    Y_train = train['target']\n",
    "    X_test = test['question_text']\n",
    "    Y_test = test['target']\n",
    "    \n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "\n",
    "def model_vectorize(data_used, vectorizer_type, binary_type, ngram, stop_word, c_val, model_type):\n",
    "    X_train = data_used[0]\n",
    "    X_test = data_used[1]\n",
    "    Y_train = data_used[2]\n",
    "    Y_test = data_used[3]\n",
    "    \n",
    "    vectorizer = vectorizer_type(binary = binary_type, stop_words = stop_word, ngram_range=ngram)\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train_vectorized = vectorizer.transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    if model_type == LogisticRegression:\n",
    "        model = model_type(C = 10**c_val)\n",
    "    else:\n",
    "        model = model_type()\n",
    "    \n",
    "    model.fit(X_train_vectorized, Y_train)\n",
    "    scores = model.predict_proba(X_test_vectorized)[:,1]\n",
    "    precision, recall, thresholds = precision_recall_curve(Y_test, scores)\n",
    "    precision, recall = precision[:-1], recall[:-1]\n",
    "    fscores = 2*np.divide(np.multiply(precision, recall), np.add(precision, recall))\n",
    "    max_fscore = np.nanmax(fscores)\n",
    "    ind_max = fscores.argmax()\n",
    "    threshold_max = thresholds[ind_max]\n",
    "    \n",
    "    return(max_fscore, threshold_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94b6703a025f7b05d8511c6a739c312103b2c5a3"
   },
   "source": [
    "##### Functions for Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e032d606b9414a9200938509051a02cb58365b7"
   },
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'wouldnt':'would not',\n",
    "                'isnt':'is not',\n",
    "                'wouldnt':'would not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'neighbour': 'neighbor',\n",
    "                'humour': 'humor',\n",
    "                'apologise': 'apologize',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'recognise': 'recognize',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'travelled': 'traveled',\n",
    "                'offence': 'offense',\n",
    "                'licence': 'license',\n",
    "                'labour':'labor',\n",
    "                'behaviour': 'behavior',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social_media',\n",
    "                'whatsapp': 'social_media',\n",
    "                'snapchat': 'social_media',\n",
    "                'facebook': 'social_media'\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    # Remove puncuation\n",
    "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"'m\", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    \n",
    "    text = re.sub('[0-9]{5,}', '#####', text)\n",
    "    text = re.sub('[0-9]{4}', '####', text)\n",
    "    text = re.sub('[0-9]{3}', '###', text)\n",
    "    text = re.sub('[0-9]{2}', '##', text)\n",
    "    text = re.sub('[0-9]{1}', '#', text)\n",
    "    \n",
    "    #Replace typical misspells\n",
    "    text = replace_typical_misspell(text)\n",
    "    \n",
    "    #Lemmitize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e824dc3c4dc066868adb6301b08902ad53b0cb7"
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "train_data = pd.read_csv('../input/train.csv', index_col = 0)\n",
    "\n",
    "#Prep data\n",
    "train_data['question_text'] = train_data[\"question_text\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ce28cd0496d95726e1adb4a74e443bedb2f4de1"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "X, y = train_data.question_text, train_data.target\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state = 0)\n",
    "cv = CountVectorizer(ngram_range=(1,4), binary=True)\n",
    "cv.fit(X_train)\n",
    "X_train, X_val = cv.transform(X_train), cv.transform(X_val)\n",
    "\n",
    "#Modeling\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "scores = lr.predict_proba(X_val)\n",
    "\n",
    "pr, re, th = metrics.precision_recall_curve(y_val, scores[:,1])\n",
    "pr, re, th = pr[:-2], re[:-2], th[:-1]\n",
    "fs = 2*np.divide(np.multiply(pr, re), np.add(pr, re))\n",
    "\n",
    "predictions = model.predict_proba(X_test_vectorized)[:,1]\n",
    "max_threshold = np.max(fs)\n",
    "\n",
    "label = []\n",
    "for i in predictions:\n",
    "    if i<= max_threshold:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)\n",
    "        \n",
    "tn, fp, fn, tp = confusion_matrix(y_val, label).ravel()\n",
    "\n",
    "print('Max Threshold: %f' %max_threshold)\n",
    "print('TN: %d, FP: %d, FN: %d, TP: %d' %(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5bcf13137717768579528159611dd5e95d71b78"
   },
   "outputs": [],
   "source": [
    "sn.set()\n",
    "sn.palplot(sn.color_palette(\"RdBu\", n_colors=7))\n",
    "plt.figure(figsize = (9, 6))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(th[np.argmax(fs)], fs[np.argmax(fs)], color = 'black', label= 'max fscore')\n",
    "plt.plot(th, pr, label = 'precision')\n",
    "plt.plot(th, re, label = 'recall')\n",
    "plt.plot(th, fs, label = 'f score')\n",
    "plt.title('Best Simple Model')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8470034fe51fedc435583f85be48a3b58aa4044"
   },
   "source": [
    "### 3. Neural Network and Embedding modeling and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91997d390a4e43b90dc187d5f85549bc6c5af3e0"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 120000\n",
    "GLOVE_DIR = \"../input/embeddings/glove.840B.300d\"\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_LEN = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ac6ac14832a2d0cb97fdd35ee37d37283987b25"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "df = pd.concat([train.drop('target',axis=1),test])\n",
    "# Comment after debugging, before committing:\n",
    "#df = df.sample(150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3735b57903804555c315d4bc01a5e05e0a92d01"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df[\"question_text\"])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embeddings_index)\n",
    "oov_glove[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \n",
    "                       \"aren't\": \"are not\",\n",
    "                       \"can't\": \"cannot\", \n",
    "                       \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\",\n",
    "                       \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \n",
    "                       \"doesn't\": \"does not\", \n",
    "                       \"don't\": \"do not\", \n",
    "                       \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\n",
    "                       \"he'll\": \"he will\", \n",
    "                       \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \n",
    "                       \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \n",
    "                       \"I'll\": \"I will\", \n",
    "                       \"I'm\": \"I am\", \n",
    "                       \"I've\": \"I have\", \n",
    "                       \"i'd\": \"i would\", \n",
    "                       \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \n",
    "                       \"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \n",
    "                       \"it'd\": \"it would\", \n",
    "                       \"it'll\": \"it will\", \n",
    "                       \"it's\": \"it is\", \n",
    "                       \"let's\": \"let us\", \n",
    "                       \"might've\": \"might have\",\n",
    "                       \"must've\": \"must have\", \n",
    "                       \"needn't\": \"need not\", \n",
    "                       \"o'clock\": \"of the clock\", \n",
    "                       \"shan't\": \"shall not\", \n",
    "                       \"she'd\": \"she would\",\n",
    "                       \"she'll\": \"she will\", \n",
    "                       \"she's\": \"she is\", \n",
    "                       \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \n",
    "                       \"that'd\": \"that would\", \n",
    "                       \"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\",\n",
    "                       \"they'll\": \"they will\", \n",
    "                       \"they're\": \"they are\", \n",
    "                       \"they've\": \"they have\", \n",
    "                       \"wasn't\": \"was not\", \n",
    "                       \"we'd\": \"we would\", \n",
    "                       \"we'll\": \"we will\", \n",
    "                       \"we're\": \"we are\", \n",
    "                       \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\", \n",
    "                       \"what'll\": \"what will\", \n",
    "                       \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \n",
    "                       \"what've\": \"what have\", \n",
    "                       \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \n",
    "                       \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \n",
    "                       \"where've\": \"where have\", \n",
    "                       \"who'll\": \"who will\", \n",
    "                       \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \n",
    "                       \"why's\": \"why is\", \n",
    "                       \"won't\": \"will not\", \n",
    "                       \"would've\": \"would have\", \n",
    "                       \"wouldn't\": \"would not\", \n",
    "                       \"y'all\": \"you all\", \n",
    "                       \"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \n",
    "                       \"you'll\": \"you will\", \n",
    "                       \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \n",
    "                       \"you've\": \"you have\" }\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "print(\"- Known Contractions -\")\n",
    "print(\"   Glove :\")\n",
    "print(known_contractions(embeddings_index))\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['treated_question'] = df['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['treated_question'])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "\n",
    "print(\"Glove :\")\n",
    "print(unknown_punct(embeddings_index, punct))\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \n",
    "                 \"₹\": \"e\", \n",
    "                 \"´\": \"'\", \n",
    "                 \"°\": \"\", \n",
    "                 \"€\": \"e\", \n",
    "                 \"™\": \"tm\", \n",
    "                 \"√\": \" sqrt \", \n",
    "                 \"×\": \"x\", \n",
    "                 \"²\": \"2\", \n",
    "                 \"—\": \"-\", \n",
    "                 \"–\": \"-\", \n",
    "                 \"’\": \"'\",\n",
    "                 \"_\": \"-\", \n",
    "                 \"`\": \"'\", \n",
    "                 '“': '\"', \n",
    "                 '”': '\"', \n",
    "                 '“': '\"', \n",
    "                 \"£\": \"e\", \n",
    "                 '∞': 'infinity', \n",
    "                 'θ': 'theta', \n",
    "                 '÷': '/', \n",
    "                 'α': 'alpha', \n",
    "                 '•': '.', \n",
    "                 'à': 'a', \n",
    "                 '−': '-', \n",
    "                 'β': 'beta', \n",
    "                 '∅': '', \n",
    "                 '³': '3', \n",
    "                 'π': 'pi'}\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['treated_question'] = df['treated_question'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "vocab = build_vocab(df['treated_question'])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embeddings_index)\n",
    "oov_glove[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {'Quorans': 'Quora',\n",
    "                'cryptocurrencies': 'cryptocurrency',\n",
    "                'Redmi' :'cellphone',\n",
    "                'OnePlus' :'cellphone',\n",
    "                'Blockchain':'blockchain',\n",
    "                'Pokémon':'Pokemon',\n",
    "                'ethereum':'Ethereum',\n",
    "                'Qoura':'Quora',\n",
    "                'fiancé':'fiance',\n",
    "                'Litecoin': 'cryptocurrency',\n",
    "                'altcoin': 'cryptocurrency',\n",
    "                'Cryptocurrency': 'cryptocurrency',\n",
    "                'altcoins' : 'cryptocurrency',\n",
    "                'litecoin' : 'cryptocurrency',\n",
    "                'colour': 'color',\n",
    "                'centre': 'center',\n",
    "                'favourite': 'favorite',\n",
    "                'travelling': 'traveling',\n",
    "                'counselling': 'counseling',\n",
    "                'theatre': 'theater',\n",
    "                'cancelled': 'canceled',\n",
    "                'labour': 'labor',\n",
    "                'organisation': 'organization',\n",
    "                'wwii': 'world war 2',\n",
    "                'citicise': 'criticize',\n",
    "                'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora',\n",
    "                'sallary': 'salary',\n",
    "                'Whta': 'What',\n",
    "                'narcisist': 'narcissist',\n",
    "                'howdo': 'how do',\n",
    "                'whatare': 'what are',\n",
    "                'howcan': 'how can',\n",
    "                'howmuch': 'how much',\n",
    "                'howmany': 'how many',\n",
    "                'whydo': 'why do',\n",
    "                'doI': 'do I',\n",
    "                'theBest': 'the best',\n",
    "                'howdoes': 'how does',\n",
    "                'mastrubation': 'masturbation',\n",
    "                'mastrubate': 'masturbate',\n",
    "                \"mastrubating\": 'masturbating',\n",
    "                'pennis': 'penis',\n",
    "                'Etherium': 'Ethereum',\n",
    "                'narcissit': 'narcissist',\n",
    "                'bigdata': 'big data', \n",
    "                '2k17': '2017', \n",
    "                '2k18': '2018', \n",
    "                'qouta': 'quota', \n",
    "                'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \n",
    "                \"whst\": 'what', 'watsapp': 'whatsapp', \n",
    "                'demonitisation': 'demonetization', \n",
    "                'demonitization': 'demonetization', \n",
    "                'demonetisation': 'demonetization'}\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "df['treated_question'] = df['treated_question'].apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "\n",
    "vocab = build_vocab(df['treated_question'])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../input/train.csv')['target']\n",
    "X = df[:len(y)]['treated_question']\n",
    "\n",
    "%%time\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "# tokenizer: words --> word indices\n",
    "\n",
    "texts, labels = X, y\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "# Example sequence\n",
    "sequences[0]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(sequences, labels, test_size=0.2)\n",
    "y_train_bin = to_categorical(y_train)\n",
    "y_val_bin = to_categorical(y_val)\n",
    "\n",
    "print('Vector for \\'apple\\' =')\n",
    "print(str(embeddings_index['apple'][0:10]) + '... plus 290 more features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# embedding_matrix: word index --> word vector\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = word_index['apple']\n",
    "print('\\'apple\\' is word number: ', ai)\n",
    "print('The vector for word number ', ai, ' is:')\n",
    "print(str(embedding_matrix[ai][0:10]) + '... plus 290 more features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Bidirectional, CuDNNGRU, GlobalMaxPool1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Input(shape=(MAX_LEN,), dtype='int32'))\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(CuDNNGRU(64, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(32, activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "#model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "from sklearn import metrics\n",
    "class cbmetrics(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        scores = self.model.predict(X_val, batch_size=1024, verbose=1)[:,1]\n",
    "        pr, re, th = metrics.precision_recall_curve(y_val, scores)\n",
    "        pr, re, th = pr[:-2], re[:-2], th[:-1]\n",
    "        fs = 2*np.divide(np.multiply(pr, re), np.add(pr, re))\n",
    "        i = np.argmax(fs)\n",
    "        self.val_f1s.append(np.max(fs))\n",
    "        self.val_recalls.append(re[i])\n",
    "        self.val_precisions.append(pr[i])\n",
    "        print(\"f score: \", np.max(fs))\n",
    "        \n",
    "f1 = cbmetrics()\n",
    "history = model.fit(X_train, y_train_bin, validation_data=(X_val, y_val_bin),\n",
    "          epochs=3, batch_size=256, verbose=1, callbacks=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, val_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(X_val, batch_size=1024, verbose=1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "pr, re, th = metrics.precision_recall_curve(y_val, scores)\n",
    "pr, re, th = pr[:-2], re[:-2], th[:-1]\n",
    "fs = 2*np.divide(np.multiply(pr, re), np.add(pr, re))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(th, pr, label = 'precision')\n",
    "plt.plot(th, re, label = 'recall')\n",
    "plt.plot(th, fs, label = 'f score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_thr = th[np.argmax(fs)]\n",
    "print(opt_thr, np.max(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
